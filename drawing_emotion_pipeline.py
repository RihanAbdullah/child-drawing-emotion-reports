# -*- coding: utf-8 -*-
"""Copy of Drawing_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uuzhjQg9rps8i6NcjJFIP59lQZbfhWZW
"""

!pip install kagglehub
!pip install transformers
!pip install torch torchvision
!pip install pillow

import kagglehub

# Download the dataset
path = kagglehub.dataset_download("vishmiperera/children-drawings")
print("Dataset downloaded to:", path)

import os

base_path = ""#Dataset path
for item in os.listdir(base_path):
    print("üìÅ", item)

import os
import pandas as pd
from PIL import Image
from transformers import pipeline
from tqdm import tqdm

# Load CLIP model (good with images+text)
classifier = pipeline("zero-shot-image-classification", model="openai/clip-vit-base-patch32")

# Human-readable prompts for drawings
prompt_labels = [
    "A drawing made by a child showing happiness",
    "A drawing made by a child showing sadness",
    "A drawing made by a child showing anger",
    "A drawing made by a child showing fear"
]

# Corresponding emotion labels for output
emotion_labels = ["Happy", "Sad", "Angry", "Fear"]

# Map verbose prompt back to class label
prompt_to_label = dict(zip(prompt_labels, emotion_labels))

# Dataset base path
base_path = ""#Dataset path
source = "data"

# Results container
results = []

# Process each labeled folder
for label in emotion_labels:
    folder_path = os.path.join(base_path, source, label)

    if not os.path.exists(folder_path):
        print(f"‚ö†Ô∏è Skipping missing folder: {folder_path}")
        continue

    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.png'))]

    for image_name in tqdm(image_files, desc=f"{source}/{label}"):
        image_path = os.path.join(folder_path, image_name)

        try:
            image = Image.open(image_path).convert("RGB")
            prediction = classifier(image, candidate_labels=prompt_labels, top_k=1)[0]

            results.append({
                "image_path": image_path,
                "actual_label": label,
                "predicted_label": prompt_to_label[prediction["label"]],
                "confidence": prediction["score"]
            })
        except Exception as e:
            print(f"‚ùå Error processing {image_path}: {e}")

df = pd.DataFrame(results)
csv_path = "/content/drive/MyDrive/drawing_emotion_predictions_prompted.csv"#Where to save the results
df.to_csv(csv_path, index=False)
print("‚úÖ Results saved to:", csv_path)

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/drawing_emotion_predictions_prompted.csv")
df.head()

accuracy = (df["actual_label"] == df["predicted_label"]).mean()
print(f"‚úÖ Prompt-based classification accuracy: {accuracy:.2%}")

# Annotate image with predicted label
annotated = image.copy()
draw = ImageDraw.Draw(annotated)

# Add label on image
label_text = f"Emotion: {predicted_emotion}"
draw.text((10, 10), label_text, fill="red")

# Show original and annotated
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Drawing")
plt.imshow(image)
plt.axis("off")

plt.subplot(1, 2, 2)
plt.title("With Classification")
plt.imshow(annotated)
plt.axis("off")
plt.show()

# Generate prompt for GPT
story_prompt = (
    f"This is a drawing made by a child. It seems to express the emotion: {predicted_emotion}. "
    "Write a short, warm, and imaginative story or interpretation explaining why the child might feel this way. "
    "Make it understandable and comforting to a parent."
)

# Use Hugging Face GPT2 if no OpenAI key
generator = pipeline("text-generation", model="gpt2-medium")
story = generator(story_prompt, max_length=100, num_return_sequences=1)[0]["generated_text"]

# Show result
print("\nüìù Generated Interpretation:\n")
print(story)

!pip install transformers
!pip install git+https://github.com/salesforce/BLIP
!pip install timm

from PIL import Image, ImageDraw, ImageFont
from transformers import pipeline, BlipProcessor, BlipForConditionalGeneration
import matplotlib.pyplot as plt
import torch
import os

# ===== Load models once =====
# Emotion classifier (CLIP)
clip_classifier = pipeline("zero-shot-image-classification", model="openai/clip-vit-base-patch32")

# Captioning model (BLIP)
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Emotion prompts and mapping
prompt_labels = [
    "A drawing made by a child showing happiness",
    "A drawing made by a child showing sadness",
    "A drawing made by a child showing anger",
    "A drawing made by a child showing fear"
]
emotion_labels = ["Happy", "Sad", "Angry", "Fear"]
prompt_to_label = dict(zip(prompt_labels, emotion_labels))


# ===== Function: Classify Emotion =====
def classify_emotion(image):
    result = clip_classifier(image, candidate_labels=prompt_labels, top_k=1)[0]
    return prompt_to_label[result["label"]], result["score"]

# ===== Function: Generate Caption =====
def caption_image(image):
    inputs = blip_processor(image, return_tensors="pt")
    out = blip_model.generate(**inputs)
    return blip_processor.decode(out[0], skip_special_tokens=True)

def interpret_child_drawing(image_path):
    # Load image
    image = Image.open(image_path).convert("RGB")

    # Step 1: Caption it
    caption = caption_image(image)

    # Step 2: Classify emotion
    emotion, confidence = classify_emotion(image)

    # Step 3: Overlay image with caption + emotion
    annotated = image.copy()
    draw = ImageDraw.Draw(annotated)
    font_size = 18
    try:
        font = ImageFont.truetype("arial.ttf", font_size)
    except:
        font = ImageFont.load_default()

    draw.text((10, 10), f"Emotion: {emotion}", fill="red", font=font)
    draw.text((10, 35), f"Caption: {caption}", fill="blue", font=font)

    # Step 4: Display
    plt.figure(figsize=(8, 6))
    plt.imshow(annotated)
    plt.axis("off")
    plt.title("Child Drawing Interpretation")
    plt.show()

    # Step 5: Final message
    print(f"üñºÔ∏è Caption: {caption}")
    print(f"üéØ Emotion: {emotion} ({confidence:.2%})")
    print(f"üìò Interpretation: This child seems to express **{emotion.lower()}**. "
          f"The drawing shows: '{caption}', which might reflect their emotional state.")

# Use this to run your full system
interpret_child_drawing("/content/f97.jpeg")

# Use this to run your full system
interpret_child_drawing("/content/h3.jpg")